{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Lecture_6_W21.ipynb","provenance":[{"file_id":"1hfKqtfW_yCevju8KtBBI7rvVx_sSLBSi","timestamp":1638279502637},{"file_id":"1mU3rdqTQotu92K1Wf7QpRkAixgNtvebU","timestamp":1622547805503},{"file_id":"1-0y76oZjRf08I8uTfvf_JdYeuY_7GRFp","timestamp":1622539608585},{"file_id":"https://github.com/IFL-CAMP/ML4MS_2020/blob/master/exercises/Exercise_5.ipynb","timestamp":1591781871348}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6-final"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"code","metadata":{"id":"r0G9KUdljfrl"},"source":["# Imports\n","# import libraries for simple image plotting and \n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import torchvision.transforms as transforms\n","import torchvision as torchvision\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","import gspread\n","from oauth2client.client import GoogleCredentials\n","from datetime import datetime\n","import pytz\n","import datetime\n","import random\n","\n","\n","tz = pytz.timezone('Europe/Berlin')\n","\n","gc = gspread.authorize(GoogleCredentials.get_application_default())\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ebg3uPH4jfrv"},"source":["student_name = \"yourName\"\n","assert student_name != \"yourName\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCD9XrpTjfsD"},"source":["#@title Result Form\n","gsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1KsS7ApUdWT1gRy20qKlRYMRV28ri0Gdb_TE-La_tD3Y/edit#gid=232421945\")\n","\n","def write_result(task_number, result=None):\n","  worksheet = gsheet.worksheet(f\"task{task_number}\")\n","  current_time = datetime.datetime.now(tz).strftime(\"%X\")\n","  current_date = str(datetime.date.today())\n","  if result:\n","    worksheet.append_row([student_name, current_time, current_date, result])\n","    print(f\"Task {task_number} successfully solved by {student_name} at {current_time} with result: {result}\")\n","  else:\n","    worksheet.append_row([student_name, current_time, current_date])\n","    print(f\"Task {task_number} successfully solved by {student_name} at {current_time}\")\n","\n","print(\"Reporting enabled - write_result(number_of_task, result='your result') \")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCSW-Zi5RIh1"},"source":["# quick example for object oriented programming: working with paths (folders and files)\n","assert student_name != \"yourName\"\n","write_result(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfvxo1CNlSec"},"source":["# Data Augmentation\n","\n","It is a common fact that medical data is scarce. But to learn a very good model, the network needs a lot of data. So to tackle the problem we perform data augmentation.\n","\n","Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. \n","\n","Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n","\n","![Data Augmentation](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n","[Source](https://cdn-images-1.medium.com/max/1000/1*C8hNiOqur4OJyEZmC7OnzQ.png) \n","\n"]},{"cell_type":"code","metadata":{"id":"kRB51WV5jfsR"},"source":["!wget https://github.com/CS4MS/CS4MS_W21/raw/main/images/cat.jpg\n","\n","from PIL import Image\n","cat = Image.open(\"cat.jpg\")\n","\n","plt.axis('off')\n","plt.imshow(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0F4QNQzjfsY"},"source":["def imshow(img):\n","    npimg = img.numpy()\n","    fig, ax = plt.subplots(figsize=(20, 20))\n","    ax.axis('off')\n","    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","def denorm(img):\n","    img[0,:,:] = (img[0,:,:] * np.asarray(norm_std[0])) + np.asarray(norm_mean[0])\n","    img[1,:,:] = (img[1,:,:] * np.asarray(norm_std[1])) + np.asarray(norm_mean[1])\n","    img[2,:,:] = (img[2,:,:] * np.asarray(norm_std[2])) + np.asarray(norm_mean[2])\n","    return img\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0LmWKewkjfsc"},"source":["augmentation = transforms.Compose([\n","                                  # resize image to the network input size\n","                                  transforms.Resize((224,224)),\n","                                  transforms.RandomHorizontalFlip(),\n","                                  transforms.RandomRotation(degrees=60),\n","                                  transforms.RandomCrop(180),\n","                                  transforms.ToTensor(),\n","                                   ])\n","# Complete list: https://pytorch.org/docs/stable/torchvision/transforms.html\n","# Examples: \n","# torchvision.transforms.RandomErasing()\n","# torchvision.transforms.RandomAffine(degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0) --> transforms.RandomAffine(degrees=20, shear=[0,50]),\n","# torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0) --> transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","# transforms.RandomCrop(180),"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qawa8dzejfsh"},"source":["images = []\n","for i in range(16):\n","    temp_im = augmentation(cat)\n","    images.append(temp_im)\n","    \n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCq6cUxSjfsj"},"source":["# Try out your own transformations \n","task1_done=False\n","own_aug =  transforms.Compose([\n","                              transforms.Resize((224,224)),\n","                              transforms.ToTensor(),\n","                              ])\n","\n","images = []\n","for i in range(16):\n","    temp_im = own_aug(cat)\n","    images.append(temp_im)  \n","# show images\n","imshow(torchvision.utils.make_grid(images))\n","\n","\n","if len(own_aug.transforms) <= 2:\n","  print(\"You have to do more than that!\")\n","else:\n","  task1_done = True\n","  print(\"task1 done!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_eVnzUYjfsn"},"source":["# Done with Task1\n","\n","Once you are happy with your augmentation submit your results\n","\n"]},{"cell_type":"code","metadata":{"id":"qGvq6UfGjfsp"},"source":["effect = \"write what the effect of your transformations is - if you want you can give it a score between 0-10\"\n","\n","\n","if task1_done and effect != \"write what the effect of your transformations is - if you want you can give it a score between 0-10\":\n","  effect += \"\\n\"+str(own_aug)\n","  write_result(1, effect)\n","else:\n","  print(\"you didnt solve task1 yet.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMKB9X3Iqekc"},"source":["## Normalization\n","Data normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network. \n","\n","Data normalization is done by subtracting the mean from each pixel and then dividing the result by the standard deviation. The distribution of such data would resemble a Gaussian curve centered at zero. \n","\n","Since, skin lesion images are natural images, we use the normalization values (mean and standard deviation) from [Imagenet dataset.](http://www.image-net.org/)\n","*norm_mean = (0.4914, 0.4822, 0.4465)*\n","\n","*norm_std = (0.2023, 0.1994, 0.2010)*\n","\n","This denotes mean and standard deviation for each channel(RGB) of an image.\n","\n","\n","We perform following data augmentation:\n","- Resize the image.\n","- Flipping the image horizontally.\n","- Randomly rotating image.\n","- Normalizing the image."]},{"cell_type":"code","metadata":{"id":"Ces4rS8jlSed"},"source":["\n","# Imagenet values\n","norm_mean = (0.4914, 0.4822, 0.4465)\n","norm_std = (0.2023, 0.1994, 0.2010)\n","\n","# define the transformaitons the images go through each time it is used for training\n","# includes augmentation AND normalization as descirbed above\n","augmentation_train = transforms.Compose([\n","                                  # resize image to the network input size\n","                                  transforms.Resize((224,224)),\n","                                  # randomly perform a horizontal flip of the image\n","                                  transforms.RandomHorizontalFlip(),\n","                                  # rotate the image with a angle from 0 to 60 (chosen randomly)\n","                                  transforms.RandomRotation(degrees=60),\n","                                  # convert the image into a tensor so it can be processed by the GPU\n","                                  transforms.ToTensor(),\n","                                  # normalize the image with the mean and std of ImageNet\n","                                  transforms.Normalize(norm_mean, norm_std),\n","                                   ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fv86Skt1qt1V"},"source":["images = []\n","for i in range(16):\n","    temp_im = augmentation_train(cat)\n","    images.append(temp_im)  \n","# show images\n","imshow(torchvision.utils.make_grid(images))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yNLSsXlYlSem"},"source":["# MedMNIST\n","\n","https://github.com/MedMNIST/MedMNIST.git\n"]},{"cell_type":"code","metadata":{"id":"61k4h0yXlSen"},"source":["!git clone https://github.com/MedMNIST/MedMNIST.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaLD7WGPCE5S"},"source":["import os\n","import sys\n","from tqdm import trange\n","from tqdm import tqdm\n","from skimage.util import montage\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","sys.path.insert(0, \"/content/MedMNIST\")\n","import MedMNIST.medmnist as medmnist\n","from MedMNIST.medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n","from MedMNIST.medmnist.evaluator import getAUC, getACC\n","from MedMNIST.medmnist.info import INFO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxF5QA_LCVn9"},"source":["print(\"Version:\", medmnist.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kZMe0UaCWhw"},"source":["data_flag = 'dermamnist'\n","download = True\n","input_root = '/content'\n","\n","flag_to_class = {\n","    \"pathmnist\": PathMNIST,\n","    \"chestmnist\": ChestMNIST,\n","    \"dermamnist\": DermaMNIST,\n","    \"octmnist\": OCTMNIST,\n","    \"pneumoniamnist\": PneumoniaMNIST,\n","    \"retinamnist\": RetinaMNIST,\n","    \"breastmnist\": BreastMNIST,\n","    \"organmnist_axial\": OrganMNISTAxial,\n","    \"organmnist_coronal\": OrganMNISTCoronal,\n","    \"organmnist_sagittal\": OrganMNISTSagittal,\n","}\n","\n","DataClass = flag_to_class[data_flag]\n","\n","info = INFO[data_flag]\n","task = info['task']\n","n_channels = info['n_channels']\n","n_classes = len(info['label'])\n","label_dict = info['label']\n","\n","print(f\"Info:\\n{info}\\n\")\n","print(f\"Task:\\n{task}\\n\")\n","print(f\"Channels:\\n{n_channels}\\n\")\n","print(f\"Number of classes:\\n{n_classes}\\n\")\n","print(f\"Label:\\n{label_dict}\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xs5L0z4UDbn2"},"source":["# no augmentation for the test data only resizing, conversion to tensor and normalization\n","augmentation_test = transforms.Compose([\n","                    transforms.Resize((224,224)),\n","                    transforms.ToTensor(),\n","                    transforms.Normalize(norm_mean, norm_std),\n","                    ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwzF3-r9lSet"},"source":["# Train, Test and Validation Split\n","It is a best practice to split the entire dataset into 3 parts:\n","- Train: Used to train a network.\n","- Validation: Fine tune the network.\n","- Test: Kept as unseen data to gauge the performance of out trained network.\n"]},{"cell_type":"code","metadata":{"id":"MaQ5wfedDOtA"},"source":["# load the data\n","train_dataset = DataClass(root=input_root, split='train', transform=augmentation_train, download=download)\n","test_dataset = DataClass(root=input_root, split='test', transform=augmentation_test, download=download)\n","val_dataset = DataClass(root=input_root, split='val', transform=augmentation_test, download=download)\n","nonorm_dataset = DataClass(root=input_root, split='train', transform=transforms.ToTensor(), download=download)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l0Vlt1dFL0O"},"source":["print(f\"Length of training dataset: {len(train_dataset)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOStN1KKFe9p"},"source":["print(train_dataset)\n","print(\"===================\")\n","print(val_dataset)\n","print(\"===================\")\n","print(test_dataset)\n","print(\"===================\")\n","print(nonorm_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kop4udupouUg"},"source":["Let's try to use the __getitem__ method of the ImageFolder class."]},{"cell_type":"code","metadata":{"id":"6i1HoAColSeq"},"source":["# Check the dimension of the 1200th image in the nonorm_dataset and its corresponding label\n","image, label = nonorm_dataset[0]\n","print(\"Image Shape: {} \\nLabel: {} \\nLesion Type: {}\".format(image.shape, label, label_dict[str(label[0])]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGNV36VPFykv"},"source":["plt.axis('off')\n","image = image.permute(1, 2, 0)\n","plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSLBtnbLtXXA"},"source":["## Task2\n","\n","sample 16 different random images from the train_dataset\n","hint: you can use the function *random.sample(range(0,10), 5)*\n","\n","save each image in the images list and print its index and label before adding it. Also add the label to the label_list.\n","hint: do it in a for loop\n","\n","finally look at what you just created"]},{"cell_type":"code","metadata":{"id":"51osdMBGrxh9"},"source":["randomlist = #code here - hint: check out random.sample\n","print(randomlist)\n","label_list = [] #This has to be filled in the for loop\n","images = [] #This has to be filled in the for loop\n","for i in randomlist:\n","    temp_im, label = train_dataset[i]\n","    print(f\"adding index: {i} with label {label}\")\n","    ## code here - hint: append is a good choice here\n","\n","# show images\n","imshow(torchvision.utils.make_grid(images))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ib3QnQFvSQm"},"source":["## Submit Task2"]},{"cell_type":"code","metadata":{"id":"-GOoSejxvtf9"},"source":["result2=\"Write here what you noticed when you looked at the visualized images and labels\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRlGzHlPvVPu"},"source":["if len(label_list) == 16 and len(images) == 16 and result2!=\"Write here what you noticed when you looked at the visualized images and labels\":\n","  result2 += \"\\n\" + \"labels: \" + str(label_list) + \"\\n\" + \"indices\" + str(randomlist)\n","  write_result(2, result2)\n","else:\n","  print(\"something didnt go as expected, check if you solved the Task2!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kSktfWdzRQC5"},"source":["# Dataloader"]},{"cell_type":"code","metadata":{"id":"x9RPKl2GMOE5"},"source":["BATCH_SIZE = 16\n","# encapsulate data into dataloader form\n","train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dUO7pdM7MtUe"},"source":["batch_images, batch_labels = next(iter(train_loader))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwJzcQXXm_gz"},"source":["# functions to show an image\n","fig = plt.figure(figsize=(20, 20))\n","\n","def denorm_imshow(img):\n","    img = denorm(img)\n","    npimg = img.numpy()\n","    plt.axis('off')\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","\n","\n","# show images\n","denorm_imshow(torchvision.utils.make_grid(batch_images))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ei4V3lwyquFz"},"source":["# Model Inference\n","Now, we have our dataset loaded Let's try to use them as input to our model.\n","\n","For now, we will use an pre trained network on a different task and do inference on the test set of out data. Let's see how is the performance without training."]},{"cell_type":"code","metadata":{"id":"YBj-R8SprHm-"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","# load a pretrained model\n","from torch import nn\n","import torchvision\n","\n","net = torchvision.models.resnet18(pretrained = True)\n","\n","# We replace last layer of resnet to match our number of classes which is 7\n","# more details next lecture\n","net.fc = nn.Linear(512, n_classes)\n","net = net.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRKNvPsFrCQ4"},"source":["# counter for correct predictions\n","correct = 0\n","# counter for all predicted samples\n","total = 0\n","\n","# set network to evaluation mode (next lecture)\n","net.eval()\n","\n","# this is for next lecture..\n","with torch.no_grad():\n","  images, labels = next(iter(train_loader))\n","  labels = torch.reshape(labels,(1,-1)).squeeze()\n","  images, labels = images.to(device), labels.to(device)\n","  outputs = net(images)\n","  _, predicted = torch.max(outputs.data, 1)\n","  total += labels.size(0)\n","  correct = (predicted == labels).sum()\n","\n","print(labels)\n","print(predicted)\n","\n","print('Accuracy of the network on the test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yRQvMJssioq0"},"source":["# Next time\n","\n","Now that we have the dataloaders and augmentations we can finally train our network so that it can actually learn to identify skin leasions."]}]}