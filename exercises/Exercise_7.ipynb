{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yOkGTUF6Kp"
      },
      "source": [
        " ## Setting up a network model and starting a first training\n",
        "\n",
        "In this and the following exercise, we are going to practise, how to set up a neural network model and perform a first training with this network.\n",
        "We will use the DermaMNIST from the MedMNIST datasets, which you have seen last lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "dnthk05cBlAV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: medmnist in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.24.1)\n",
            "Requirement already satisfied: scikit-image in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.19.0)\n",
            "Requirement already satisfied: Pillow in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (8.2.0)\n",
            "Requirement already satisfied: torch in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.10.0)\n",
            "Requirement already satisfied: torchvision in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.11.1)\n",
            "Requirement already satisfied: tqdm in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (4.61.2)\n",
            "Requirement already satisfied: fire in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (0.4.0)\n",
            "Requirement already satisfied: pandas in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.2.5)\n",
            "Requirement already satisfied: numpy in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from medmnist) (1.20.2)\n",
            "Requirement already satisfied: termcolor in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from fire->medmnist) (1.1.0)\n",
            "Requirement already satisfied: six in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from pandas->medmnist) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from pandas->medmnist) (2021.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (1.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (20.9)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2021.7.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-image->medmnist) (1.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from packaging>=20.0->scikit-image->medmnist) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-learn->medmnist) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from scikit-learn->medmnist) (2.2.0)\n",
            "Requirement already satisfied: typing_extensions in /Users/mmenten/miniconda3/envs/default/lib/python3.8/site-packages (from torch->medmnist) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "# from tqdm import trange\n",
        "# from tqdm import tqdm\n",
        "# from skimage.util import montage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision as torchvision\n",
        "\n",
        "!pip install medmnist\n",
        "import medmnist\n",
        "from medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n",
        "from medmnist.evaluator import getAUC, getACC\n",
        "from medmnist.info import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MGpKZ1TFwfnL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "print(\"Version:\", medmnist.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "x6nPfDx8wiAR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: tmp_data: File exists\n",
            "Info:\n",
            "{'python_class': 'DermaMNIST', 'description': 'The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.', 'url': 'https://zenodo.org/record/5208230/files/dermamnist.npz?download=1', 'MD5': '0744692d530f8e62ec473284d019b0c7', 'task': 'multi-class', 'label': {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}, 'n_channels': 3, 'n_samples': {'train': 7007, 'val': 1003, 'test': 2005}, 'license': 'CC BY-NC 4.0'}\n",
            "\n",
            "Task:\n",
            "multi-class\n",
            "\n",
            "Channels:\n",
            "3\n",
            "\n",
            "Number of classes:\n",
            "7\n",
            "\n",
            "Label:\n",
            "{'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# various MedMNIST datasets\n",
        "data_flag = 'dermamnist'\n",
        "download = True\n",
        "input_root = 'tmp_data/'\n",
        "!mkdir 'tmp_data'\n",
        "\n",
        "flag_to_class = {\n",
        "    \"pathmnist\": PathMNIST,\n",
        "    \"chestmnist\": ChestMNIST,\n",
        "    \"dermamnist\": DermaMNIST,\n",
        "    \"octmnist\": OCTMNIST,\n",
        "    \"pneumoniamnist\": PneumoniaMNIST,\n",
        "    \"retinamnist\": RetinaMNIST,\n",
        "    \"breastmnist\": BreastMNIST,\n",
        "    \"organmnist_axial\": OrganMNISTAxial,\n",
        "    \"organmnist_coronal\": OrganMNISTCoronal,\n",
        "    \"organmnist_sagittal\": OrganMNISTSagittal,\n",
        "}\n",
        "\n",
        "DataClass = flag_to_class[data_flag]\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "label_dict = info['label']\n",
        "\n",
        "print(f\"Info:\\n{info}\\n\")\n",
        "print(f\"Task:\\n{task}\\n\")\n",
        "print(f\"Channels:\\n{n_channels}\\n\")\n",
        "print(f\"Number of classes:\\n{n_classes}\\n\")\n",
        "print(f\"Label:\\n{label_dict}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2I6F49b8fRx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Defining the augmentations\n",
        "\n",
        "As described in the last exercise, we now define the augmentations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HjF-Aq9BF6K_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Imagenet values\n",
        "norm_mean = (0.4914)\n",
        "norm_std = (0.2023)\n",
        "\n",
        "# define the transformaitons the images go through each time it is used for training\n",
        "# includes augmentation AND normalization as described above\n",
        "augmentation_train = transforms.Compose([\n",
        "                                  # resize image to the network input size\n",
        "                                  transforms.Resize((28,28)),\n",
        "                                  # rotate the image with a certain angle range, randomly chosen\n",
        "                                  transforms.RandomRotation(degrees=20),\n",
        "                                  # convert the image into a tensor so it can be processed by the GPU\n",
        "                                  transforms.ToTensor(),\n",
        "                                  # normalize the image with the mean and std of ImageNet\n",
        "                                  transforms.Normalize(norm_mean, norm_std),\n",
        "                                   ])\n",
        "\n",
        "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
        "augmentation_test = transforms.Compose([\n",
        "                    transforms.Resize((28,28)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(norm_mean, norm_std),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGR6pIuMJsqK"
      },
      "source": [
        "## Splitting up data\n",
        "\n",
        "Set up datasets for training, validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "AHYTu04_yDvo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://zenodo.org/record/5208230/files/dermamnist.npz?download=1 to tmp_data/dermamnist.npz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "19725312it [00:17, 1108541.50it/s]                              \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n",
            "Using downloaded and verified file: tmp_data/dermamnist.npz\n"
          ]
        }
      ],
      "source": [
        "# load the data\n",
        "train_dataset = DataClass(root=input_root, split='train', transform=augmentation_train, download=download)\n",
        "test_dataset = DataClass(root=input_root, split='test', transform=augmentation_test, download=download)\n",
        "val_dataset = DataClass(root=input_root, split='val', transform=augmentation_test, download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "zL9p3c52xpUB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 7007\n",
            "    Root location: tmp_data/\n",
            "    Split: train\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 1003\n",
            "    Root location: tmp_data/\n",
            "    Split: val\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n",
            "===================\n",
            "Dataset DermaMNIST (dermamnist)\n",
            "    Number of datapoints: 2005\n",
            "    Root location: tmp_data/\n",
            "    Split: test\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'actinic keratoses and intraepithelial carcinoma', '1': 'basal cell carcinoma', '2': 'benign keratosis-like lesions', '3': 'dermatofibroma', '4': 'melanoma', '5': 'melanocytic nevi', '6': 'vascular lesions'}\n",
            "    Number of samples: {'train': 7007, 'val': 1003, 'test': 2005}\n",
            "    Description: The DermaMNIST is based on the HAM10000, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. The dataset consists of 10,015 dermatoscopic images categorized as 7 different diseases, formulized as a multi-class classification task. We split the images into training, validation and test set with a ratio of 7:1:2. The source images of 3×600×450 are resized into 3×28×28.\n",
            "    License: CC BY-NC 4.0\n"
          ]
        }
      ],
      "source": [
        "# Some detailed information about all splits\n",
        "print(\"===================\")\n",
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(val_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZXPqSvKNyh"
      },
      "source": [
        "## Create Dataloaders\n",
        "\n",
        "PyTorch provides template Dataloader classes for easy data handling, assigning according transforms and splits. You can find more information about how PyTorch handles datasets and data loading [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VUURp9J01EtJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "### encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NMfm-GkQyFe3"
      },
      "outputs": [],
      "source": [
        "### the next() function returns the next item from the iterator.\n",
        "batch_images, batch_labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "7O9BTWgOyIhK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fbd73b02370>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# show all 3 channels of image 10\n",
        "print(batch_images[0].shape)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][0,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][1,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][2,:,:])\n",
        "\n",
        "### different color maps\n",
        "### cmap='bone', cmap = 'summer', cmap = 'seismic'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3QclrDwU6j2B"
      },
      "source": [
        "## Define a Convolutional Neural Network\n",
        "\n",
        "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from [torch library](https://pytorch.org/docs/stable/nn.html).\n",
        "\n",
        "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authors’ implementation of LeNet was used primarily for OCR and character recognition in documents. The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs.\n",
        "\n",
        "To define a neural network in PyTorch one has to create a class inhereting from [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Crucially, this class has to include the function forward() which defines which computation should be performed at every call given a batch of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "C4SswIxD6j2B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(classes)\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=(5,5), padding=2)\n",
        "        self.nonlin1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5)) \n",
        "        self.nonlin2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2))\n",
        "        \n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.nonlin3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.nonlin4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "        self.nonlin5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.conv1(x)\n",
        "        x = self.nonlin1(x)\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.nonlin2(x)\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.nonlin3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.nonlin4(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.nonlin5(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        \n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use torchsummary to print out the architecture of your model given a certain input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [128, 6, 28, 28]             156\n",
            "              ReLU-2           [128, 6, 28, 28]               0\n",
            "         MaxPool2d-3           [128, 6, 14, 14]               0\n",
            "            Conv2d-4          [128, 16, 10, 10]           2,416\n",
            "              ReLU-5          [128, 16, 10, 10]               0\n",
            "         MaxPool2d-6            [128, 16, 5, 5]               0\n",
            "            Linear-7                 [128, 120]          48,120\n",
            "              ReLU-8                 [128, 120]               0\n",
            "            Linear-9                  [128, 84]          10,164\n",
            "             ReLU-10                  [128, 84]               0\n",
            "           Linear-11                   [128, 7]             595\n",
            "             ReLU-12                   [128, 7]               0\n",
            "================================================================\n",
            "Total params: 61,451\n",
            "Trainable params: 61,451\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.38\n",
            "Forward/backward pass size (MB): 14.26\n",
            "Params size (MB): 0.23\n",
            "Estimated Total Size (MB): 14.88\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "summary(LeNet(), input_size=(1,28,28), batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf87hhWCaqp"
      },
      "source": [
        "## Homework\n",
        "\n",
        "Create a neural network capable of processing the image tensor 'img', which has only one channel. The network should contain at least one convolutional layer and one additional fully connected layer. Pay attention that within the fully connected layer the output dimension of the last convolutional layer has to fit the input dimension. Additionally, the output dimension of the fully connected layer before 'fc_fin' has to match the required input dimension of 'fc_fin'.\n",
        "\n",
        "Some hints:\n",
        "\n",
        "- Have a look at how LeNet is implemented above. Many concepts can be copied.\n",
        "- Focus on using the PyTorch's Linear, Conv2d, MaxPool2d and ReLU layers and the .view() function. You can find detailed information on these components in [PyTorch's documentation](https://pytorch.org/docs/stable/index.html).\n",
        "- Batch size does not have to be considered when designing the neural networks. PyTorch adapts the calculations automatically when provided with different batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3ia2S8hrDCIL"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (200x200 and 48x6)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-ed94fdf8a41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/default/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-ed94fdf8a41f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# --------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_fin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/default/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/default/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/default/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (200x200 and 48x6)"
          ]
        }
      ],
      "source": [
        "img = torch.rand((1, 1, 200, 200))\n",
        "\n",
        "output_dim = 6\n",
        "\n",
        "class LeNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet2, self).__init__()\n",
        "\n",
        "        # --------------------\n",
        "\n",
        "        ########## Insert your layers here ##########\n",
        "\n",
        "        # --------------------\n",
        "\n",
        "        self.fc_fin = nn.Linear(in_features=48, out_features=output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # --------------------\n",
        "\n",
        "        ########## Insert your forward pass here ##########\n",
        "\n",
        "        # --------------------\n",
        "\n",
        "        x = self.fc_fin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "model = LeNet2()\n",
        "\n",
        "output = model(img)\n",
        "\n",
        "if output.size(1) == 6:\n",
        "    print('The correct output size has been generated.')\n",
        "else:\n",
        "    print('The generated output size is not correct')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Exercise_7_solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
