{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5yOkGTUF6Kp"
      },
      "source": [
        " # 5.2 Setting up a network model and starting a first training\n",
        "\n",
        "In this exercise, we are going to practise, how to set up a neural network model and perform a first training with this network.\n",
        "We will use the DermaMNIST from the MedMNIST datasets, which you have seen last lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcsMXhTYyw3w"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MedMNIST/MedMNIST.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnthk05cBlAV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "!pip install --upgrade -q gspread\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "tz = pytz.timezone('Europe/Berlin')\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# from tqdm import trange\n",
        "# from tqdm import tqdm\n",
        "# from skimage.util import montage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision as torchvision\n",
        "\n",
        "import MedMNIST.medmnist as medmnist\n",
        "from MedMNIST.medmnist.dataset import PathMNIST, ChestMNIST, DermaMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, OrganMNISTAxial, OrganMNISTCoronal, OrganMNISTSagittal\n",
        "from MedMNIST.medmnist.evaluator import getAUC, getACC\n",
        "from MedMNIST.medmnist.info import INFO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGpKZ1TFwfnL"
      },
      "outputs": [],
      "source": [
        "print(\"Version:\", medmnist.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6nPfDx8wiAR"
      },
      "outputs": [],
      "source": [
        "# various MedMNIST datasets\n",
        "\n",
        "data_flag = 'dermamnist'\n",
        "download = True\n",
        "input_root = 'tmp_data/'\n",
        "\n",
        "flag_to_class = {\n",
        "    \"pathmnist\": PathMNIST,\n",
        "    \"chestmnist\": ChestMNIST,\n",
        "    \"dermamnist\": DermaMNIST,\n",
        "    \"octmnist\": OCTMNIST,\n",
        "    \"pneumoniamnist\": PneumoniaMNIST,\n",
        "    \"retinamnist\": RetinaMNIST,\n",
        "    \"breastmnist\": BreastMNIST,\n",
        "    \"organmnist_axial\": OrganMNISTAxial,\n",
        "    \"organmnist_coronal\": OrganMNISTCoronal,\n",
        "    \"organmnist_sagittal\": OrganMNISTSagittal,\n",
        "}\n",
        "\n",
        "DataClass = flag_to_class[data_flag]\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "label_dict = info['label']\n",
        "\n",
        "print(f\"Info:\\n{info}\\n\")\n",
        "print(f\"Task:\\n{task}\\n\")\n",
        "print(f\"Channels:\\n{n_channels}\\n\")\n",
        "print(f\"Number of classes:\\n{n_classes}\\n\")\n",
        "print(f\"Label:\\n{label_dict}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP9neANR56LG"
      },
      "source": [
        "The first time you run this there is probably an error. Just click on \"RUNTIME\" --> \"RESTART RUNTIME\" and run this cell again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n5esauf6O7l",
        "outputId": "470ae046-c3d7-4847-8956-9fab0df710f9",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reporting enabled - write_result(number_of_task, result='your result') \n"
          ]
        }
      ],
      "source": [
        "# @title Result Form\n",
        "gsheet = gc.open_by_url(\"https://docs.google.com/spreadsheets/d/1oNM74xlAKuSJESKm2ggO5sduKYdVab4xzTShiG0lB34/edit?usp=sharing\")\n",
        "\n",
        "def write_result(task_number, result=None):\n",
        "  worksheet = gsheet.worksheet(\"task{}\".format(task_number))\n",
        "  current_time = datetime.datetime.now(tz).strftime(\"%X\")\n",
        "  current_date = str(datetime.date.today())\n",
        "  if result:\n",
        "    worksheet.append_row([student_name, current_time, current_date, result])\n",
        "    print(\"Task {} successfully solved by {} at {} with result: {}\".format(task_number, student_name, current_time, result))\n",
        "  else:\n",
        "    worksheet.append_row([student_name, current_time, current_date])\n",
        "    print(\"Task {} successfully solved by {} at {}\".format(task_number, student_name, current_time))\n",
        "\n",
        "print(\"Reporting enabled - write_result(number_of_task, result='your result') \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM9BQxPI6Eef"
      },
      "outputs": [],
      "source": [
        "student_name = \"FlorianH\"\n",
        "assert student_name != \"yourName\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2I6F49b8fRx",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Defining the augmentations\n",
        "\n",
        "As described in the last exercise, we now define the augmentations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjF-Aq9BF6K_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Imagenet values\n",
        "norm_mean = (0.4914)\n",
        "norm_std = (0.2023)\n",
        "\n",
        "# define the transformaitons the images go through each time it is used for training\n",
        "# includes augmentation AND normalization as described above\n",
        "augmentation_train = transforms.Compose([\n",
        "                                  # resize image to the network input size\n",
        "                                  transforms.Resize((28,28)),\n",
        "                                  # rotate the image with a certain angle range, randomly chosen\n",
        "                                  transforms.RandomRotation(degrees=20),\n",
        "                                  # convert the image into a tensor so it can be processed by the GPU\n",
        "                                  transforms.ToTensor(),\n",
        "                                  # normalize the image with the mean and std of ImageNet\n",
        "                                  transforms.Normalize(norm_mean, norm_std),\n",
        "                                   ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS7bZZuOF6LE"
      },
      "outputs": [],
      "source": [
        "# no augmentation for the test data only resizing, conversion to tensor and normalization\n",
        "augmentation_test = transforms.Compose([\n",
        "                    transforms.Resize((28,28)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(norm_mean, norm_std),\n",
        "                    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSnRNgR68WQ-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We are again using a form to monitor the progress for the different tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I39Sd9NJOvj7"
      },
      "outputs": [],
      "source": [
        "# Confirm that you are ready to go:\n",
        "write_result(0, 'Ready!!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGR6pIuMJsqK"
      },
      "source": [
        "set up datasets for training, validation and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHYTu04_yDvo"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "train_dataset = DataClass(root=input_root, split='train', transform=augmentation_train, download=download)\n",
        "test_dataset = DataClass(root=input_root, split='test', transform=augmentation_test, download=download)\n",
        "val_dataset = DataClass(root=input_root, split='val', transform=augmentation_test, download=download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IREfhdvmx3Q4"
      },
      "outputs": [],
      "source": [
        "### length of training dataset\n",
        "print(f\"Length of training dataset: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL9p3c52xpUB"
      },
      "outputs": [],
      "source": [
        "### and some more detailed information about all splts\n",
        "print(\"===================\")\n",
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(val_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPZXPqSvKNyh"
      },
      "source": [
        "create dataloaders for easy data handling, assigning according transforms and splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUURp9J01EtJ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "### encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMfm-GkQyFe3"
      },
      "outputs": [],
      "source": [
        "### the next() function returns the next item from the iterator.\n",
        "batch_images, batch_labels = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O9BTWgOyIhK"
      },
      "outputs": [],
      "source": [
        "# show all 3 channels of image 10\n",
        "print(batch_images[0].shape)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][0,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][1,:,:])\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.imshow(batch_images[11][2,:,:])\n",
        "\n",
        "### different color maps\n",
        "### cmap='bone', cmap = 'summer', cmap = 'seismic'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB0u2jdsF6LK"
      },
      "outputs": [],
      "source": [
        "# get the total amount of images in the dataset\n",
        "num_train = len(train_dataset)\n",
        "\n",
        "# create a list of indices for the whole dataset\n",
        "indices = list(range(num_train))\n",
        "\n",
        "# get the class labels from the dataset object (0-6)\n",
        "class_labels = train_dataset.label\n",
        "classes = [0, 1, 2, 3, 4, 5, 6]\n",
        "# define the percentage of data that is not used for training\n",
        "split_size = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3QclrDwU6j2B"
      },
      "source": [
        "# Define a Convolutional Neural Network\n",
        "\n",
        "Pytorch makes it very easy to define a neural network. We have layers like Convolutions, ReLU non-linearity, Maxpooling etc. directly from torch library.\n",
        "\n",
        "In this tutorial, we use The LeNet architecture introduced by LeCun et al. in their 1998 paper, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf). As the name of the paper suggests, the authorsâ€™ implementation of LeNet was used primarily for OCR and character recognition in documents.\n",
        "\n",
        "The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4SswIxD6j2B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(classes)\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5,5)) \n",
        "        self.fc1   = nn.Linear(16*5*5, 120)\n",
        "        self.fc2   = nn.Linear(120, 84)\n",
        "        self.fc3   = nn.Linear(84, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hf87hhWCaqp"
      },
      "source": [
        "## Task 1\n",
        "Create a neural network capable of processing the image tensor 'img', which has only one channel. The network should contain at least one convolutional layer and one additional fully connected layer. Pay attention that within the fully connected layer the output dimension of the last convolutional layer has to fit the input dimension. Additionally, the output dimension of the fully connected layer before 'fc_fin' has to match the required input dimension of 'fc_fin'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ia2S8hrDCIL"
      },
      "outputs": [],
      "source": [
        "img = torch.rand((1, 1, 200, 200))\n",
        "\n",
        "output_dim = 6\n",
        "\n",
        "class LeNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet2, self).__init__()\n",
        "        # --------------------\n",
        "        ########## Insert your layers here ##########\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5,5), padding=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5,5)) \n",
        "        self.fc1   = nn.Linear(16*48*48, 120)\n",
        "        self.fc2   = nn.Linear(120, 64)\n",
        "\n",
        "        # --------------------\n",
        "        self.fc_fin = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # --------------------\n",
        "        ########## Insert your forward pass here ##########\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # --------------------\n",
        "        x = self.fc_fin(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "model = LeNet2()\n",
        "\n",
        "task_done = False\n",
        "output = model(img)\n",
        "\n",
        "if output.size(1) == 6:\n",
        "  print('The correct output size has been generated.')\n",
        "  task_done = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aotjbdShGg17"
      },
      "source": [
        "When your model processes the image correctly, submit your result by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5-gqjtwF9mH"
      },
      "outputs": [],
      "source": [
        "if task_done:\n",
        "  write_result(1, str(model))\n",
        "else:\n",
        "  print(\"You didnt solve the task yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_R_SSWF6Lb"
      },
      "source": [
        "# Define a Loss function\n",
        "\n",
        "Let's use a Classification Cross-Entropy loss.\n",
        "\n",
        "$H_{y'} (y) := - \\sum_{i} y_{i}' \\log (y_i)$\n",
        "\n",
        "### Median Frequency Balancing\n",
        "There are datasets which have a large imbalance in the amount of label occurrence. A prediction would be therefore biased towards stronger represented classes. As a solution, we use **Median Frequency Balancing**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCJn-DV5F6Lc"
      },
      "outputs": [],
      "source": [
        "# Median Frequency Balancing\n",
        "\n",
        "# get the class labels of each image\n",
        "class_labels = train_dataset.label\n",
        "# empty array for counting instance of each class\n",
        "count_labels = np.zeros(len(classes))\n",
        "# empty array for weights of each class\n",
        "class_weights = np.zeros(len(classes))\n",
        "\n",
        "# populate the count array\n",
        "for l in class_labels:\n",
        "  count_labels[l] += 1\n",
        "\n",
        "# get median count\n",
        "median_freq = np.median(count_labels)\n",
        "\n",
        "# calculate the weigths\n",
        "for i in range(len(classes)):\n",
        "  class_weights[i] = median_freq/count_labels[i]\n",
        "\n",
        "# print the weights\n",
        "for i in range(len(classes)):\n",
        "    print(classes[i],\":\", class_weights[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pPUGMq3NCnP"
      },
      "source": [
        "Now we define the loss function with the weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-Q72RJ-M84P"
      },
      "outputs": [],
      "source": [
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight = class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRBdC31sF6Li"
      },
      "source": [
        "# Define the Optimizer\n",
        "\n",
        "The most common and effective Optimizer currently used is **Adam: Adaptive Moments**. You can look [here](https://arxiv.org/abs/1412.6980) for more information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmMjYnCjF6Lk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# now lets go back to the initial LeNet architecture\n",
        "net = LeNet()\n",
        "net = net.to(device)\n",
        "\n",
        "# and define an optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oGr-AfqF6Ln"
      },
      "source": [
        "# Training\n",
        "\n",
        "After everything has been set up, we can now start an actual training on our MNIST dataset. To save time, for the moment we will run only ten epochs. Within the training, our dataloader is used to load a batch from our MNIST dataset. This batch is forwarded to the model. The corresponding output is compared against its labels with the chosen loss function, here called 'criterion'. Then, the loss values are backpropagated through the whole model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyc3or5oF6Lo"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "def run_epoch():\n",
        "  running_loss = 0.0\n",
        "  data_loader = train_loader\n",
        "  for i, data in enumerate(data_loader):\n",
        "      # get the inputs\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), torch.argmax(labels, 1).to(device)\n",
        "      \n",
        "      # set the parameter gradients to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(inputs[:,0:1,:,:])\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      #compute accuracy\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      running_loss += loss.item()\n",
        "  return running_loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    running_loss = run_epoch()\n",
        "\n",
        "    running_loss /= len(train_loader)\n",
        "\n",
        "    print('Epoch: {}'.format(epoch+1))\n",
        "    print('Loss: {}' .format(running_loss))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRfEARzBKd_U"
      },
      "source": [
        "## Task 2\n",
        "Rewrite the 'run_epoch' function, so that it can perform the training and validation in consecutive steps for each epoch. By using the corresponding dataloaders, the data can be easily provided to the model. The train() and eval() function change the model into the corresponding state. Please note that by running the previous code box the model is now already trained. We therefore re-initialize the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahiXcT1zLlpU"
      },
      "outputs": [],
      "source": [
        "net = LeNet()\n",
        "net = net.to(device)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=1e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "def run_epoch():\n",
        "  loss_dict = {'train': 0.0,\n",
        "               'val': 0.0}\n",
        "\n",
        "  ########## Specifiy which dataloader needs to be used in a phase within a dictionary ##########\n",
        "  data_loader = {'train': train_loader,\n",
        "                  'val': val_loader}\n",
        "\n",
        "  ########## ----------------------- ##########\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "\n",
        "    running_loss = 0.0\n",
        "    num_correct = 0\n",
        "    num_all = 0\n",
        "\n",
        "    for i, data in enumerate(data_loader[phase]):\n",
        "\n",
        "      ########## get the inputs ##########\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), torch.argmax(labels, 1).to(device)\n",
        "      ########## ------------------------------- ##########\n",
        "\n",
        "      # set the parameter gradients to zero\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if phase == 'train':\n",
        "        net.train()  # Set model to training mode\n",
        "      else:\n",
        "        net.eval()  # Set model to validation mode\n",
        "\n",
        "      # Calculate the network output and its loss \n",
        "      outputs = net(inputs[:,0:1,:,:])\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # The loss backpropagation and optimization step is only necessary when\n",
        "      # in training mode:\n",
        "      # -------------------------------\n",
        "      ########## Implement the loss backpropagation for the training phase only ##########\n",
        "      if phase == 'train':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      ########## ------------------------------- ##########\n",
        "      \n",
        "      #compute accuracy\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      if phase == 'val':\n",
        "        num_correct += torch.sum(predicted == labels).item()\n",
        "        num_all += labels.size()[0]\n",
        "\n",
        "    # ----------------------------------\n",
        "    ########## Save the correct loss for every phase within the dictionary ##########\n",
        "    loss_dict[phase] = running_loss / len(data_loader[phase])\n",
        "    ########## ------------------------------- ##########\n",
        "  accuracy = num_correct/num_all\n",
        "  return loss_dict, accuracy\n",
        "\n",
        "run_through = False\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    loss_dict, accuracy = run_epoch()\n",
        "\n",
        "    print('Epoch: {}'.format(epoch+1))\n",
        "    for phase in ['train', 'val']:\n",
        "      print('Loss {}: {}' .format(phase, loss_dict[phase]))\n",
        "      if phase == 'val':\n",
        "        print('Validation Accuracy: {}%' .format(np.round(accuracy, 3)*100))\n",
        "    print()\n",
        "    run_through = True\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPv7urj9AddW"
      },
      "source": [
        "After you have sucessfully performed the training, please submit your results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3WP2f1mYIZ4"
      },
      "outputs": [],
      "source": [
        "if run_through:\n",
        "  write_result(2, 'The last obtained validation loss is {}'.format(loss_dict['val']))\n",
        "else:\n",
        "  print(\"You didnt solve the task yet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WTJyWOuAcwi"
      },
      "source": [
        "The following function is a useful tool to get information about your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwjnpkM5HDBO"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(net, input_size=(1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIbJukXZA7ap"
      },
      "source": [
        "# Homework\n",
        "After performing the training and validation of your system you are now ready to perform the inference on your test set. Implement the inference step for out dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFoJ0C9OBYqY"
      },
      "outputs": [],
      "source": [
        "running_loss = 0.0\n",
        "num_correct = 0\n",
        "num_all = 0\n",
        "data_loader = test_loader\n",
        "for i, data in enumerate(data_loader):\n",
        "\n",
        "  ########## Implement the data loading and prediction on the test set ##########\n",
        "  \n",
        "  # your code here\n",
        "\n",
        "  ########## ------------------------------- ##########\n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  num_correct += torch.sum(predicted == torch.argmax(labels, 1)).item()\n",
        "  num_all += labels.size()[0]\n",
        "  running_loss += loss.item()\n",
        "running_loss /= len(data_loader)\n",
        "\n",
        "print('Loss for test set is {}'.format(running_loss))\n",
        "print('Test accuracy of the network: {}%'.format(np.round(num_correct/num_all*100, 3)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujoTLsvfC8Vk"
      },
      "source": [
        "After you completed the inference, submit your test loss result here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZxDwplvCVUG"
      },
      "outputs": [],
      "source": [
        "if not running_loss == 0.0:\n",
        "  write_result(3, 'The obtained test loss is {}'.format(running_loss))\n",
        "else:\n",
        "  print(\"You didnt solve the task yet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha_1vCh2bevl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Exercise_7_solution.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
